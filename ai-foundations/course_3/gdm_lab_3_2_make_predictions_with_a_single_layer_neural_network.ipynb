{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DAsej7XSlYt"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myzQnLMOJ91"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C3-white-bg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE0jaJsaICiX"
      },
      "source": [
        "# Lab: Make Predictions with a Single-Layer Neural Network\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_3/gdm_lab_3_2_make_predictions_with_a_single_layer_neural_network.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Separate embeddings into two classes with a single-layer neural network.\n",
        "\n",
        "15 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSYq3aNrdpnC"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will gain an intuitive understanding of how neural networks and multi-layer perceptrons work by investigating how a single artificial neuron makes decisions. You will see how a separating line can classify two classes of 2D data points. In the dataset provided, each datapoint corresponds to the embedding of a prompt. The classifier's task is to predict the next token from the two tokens \"water\" and \"food\".\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Qf3eGhfl4H"
      },
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will:\n",
        "* Understand how data can be separated for performing binary classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIhcVxx0foVo"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "Your task is to adjust the weights and bias of a single artificial neuron to find a decision boundary that separates the two classes. This neuron uses a sigmoid activation function to compute the probability of class 1. This type of classifier is also known as a logistic regression classifier, which you may have previously encountered if you have ever taken a statistics course.\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Load the dataset of 2-dimensional embeddings.\n",
        "* Visualize the dataset.\n",
        "* Manually adjust the weights and the bias term until the result decision boundary perfectly separates the two classes.\n",
        "\n",
        "All of these steps are described in detail in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNG_jg-39Zj"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in *cells* that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtgZxrpjm6j"
      },
      "source": [
        "Note that the order in which you run the cells matters. When you are working through a lab, make sure to always run all cells in order. Otherwise, the code might not work. If you take a break while working on a lab, Colab may disconnect you. In that case, you have to execute all cells again before continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-d1hd7Xndke"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will use the `numpy` module of the `JAX` package for defining vectors. This package implements almost the same functions as the `numpy` module which you have encountered in the previous course. However, this has been optimized for defining data and parameters for neural networks. As you will see in later modules of this course, it also comes with additional functionality that makes it easier to train neural networks. For now, think of it exactly the same way as `numpy` but instead of prefixing everything with `np`, prefix everything with `jnp` (for `jax.numpy`). This lab also uses `pandas` for loading and displaying the dataset and the custom `ai_foundations` package for creating plots.\n",
        "\n",
        "Run the following cell to import all required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPy-Uxih7Xuz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "# Packages used.\n",
        "import jax.numpy as jnp # For defining vectors and matrices.\n",
        "import pandas as pd # For loading and displaying the dataset.\n",
        "from ai_foundations import visualizations # For creating plots.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjlxkgirQANA"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "You will work with a small dataset which contains the embeddings for predicting the next token for 20 different prompts. These **prompt embeddings** work similarly as the token embeddings that you have seen in the previous course. However, the idea behind prompt embeddings is that they encode not only the information of an individual token, but also the information of an entire prompt. You will learn in the next course (04 Discover the Transformer Architecture), how you can use transformer models to compute such prompt embeddings. For now, assume that there are methods to obtain such prompt embeddings and that the output of this method are the 2D datapoints in the dataset that you are loading.\n",
        "\n",
        "In this dataset, for half of the prompt embeddings, the likely next token is \"food\", and for the other half the likely next token is \"water\". The goal of the classifier you are building in this lab is to correctly predict the next token.\n",
        "\n",
        "Run the following cell to download the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOvUXJfZP-7C"
      },
      "outputs": [],
      "source": [
        "# Load data using pandas.\n",
        "df = pd.read_csv(\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/food-water-dataset.csv\")\n",
        "\n",
        "# Extract embeddings (Embedding_dim_1, Embedding_dim_2) and labels.\n",
        "# These are represented as a JAX array using the jnp.array function.\n",
        "input_features = jnp.array(df[[\"Embedding_dim_1\", \"Embedding_dim_2\"]].values)\n",
        "labels = df[\"Label\"].values # Labels: \"food\" or \"water\".\n",
        "\n",
        "# Convert labels to numeric values for plotting (food = 1, water = 0).\n",
        "numeric_labels = jnp.where(labels == \"food\", 1, 0)\n",
        "\n",
        "# Print the loaded data for verification.\n",
        "df.head(n=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joKZ4SlRQ1Do"
      },
      "source": [
        "### Plot the data points\n",
        "\n",
        "As a first step, run the following cell to plot the two-dimensional embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6juEoRsQ0qw"
      },
      "outputs": [],
      "source": [
        "visualizations.plot_data_and_decision_boundary(\n",
        "    input_features, labels, title=\"2D Prompt Embeddings\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7MgQK73QSqg"
      },
      "source": [
        "The blue dots correspond to prompt embeddings for which the next token should be \"water\" and the yellow dots correspond to prompt embeddings for which the next token should be \"food\". All of the blue dots are quite close together and the same is true for all of the yellow dots. This enables you to build a classifier that can separate the classes with high accuracy. This allows you to build a language model that can adequately predict the next token from this list of two possible tokens.\n",
        "\n",
        "Also note that all of these data points are treated as training examples, so currently there is no separate **test set**. In later labs, you will automatically train models and for those labs, you will first split the dataset into a training and test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFpTVIYhXkBC"
      },
      "source": [
        "## Coding Activity 1: Tune the weight and bias manually\n",
        "\n",
        "Recall from the previous article that a single layer of a multi-layer perceptron (MLP) computes a weighted sum of the input vector $\\mathbf{x}$ by computing the dot product between $\\mathbf{x}$ and the weight vector $\\mathbf{w}$. The final output $y$ is then computed by adding the bias term $b$ and applying a non-linear activation function, such as the sigmoid function $\\sigma$:\n",
        "\n",
        "$$ y = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b).$$\n",
        "\n",
        "In this activity, you will manually adjust the weight vector $\\mathbf{w}$ and the bias term $b$, such that the resulting classifier perfectly separates the two classes in your dataset.\n",
        "\n",
        "Remember that the sigmoid function returns a value greater than 0.5 for positive values and a value smaller than 0.5 for negative values. Therefore, your classifier will work correctly if $\\mathbf{w} \\cdot \\mathbf{x} + b > 0$ for all data points of the positive class (here arbitrarily chosen to be \"food\") and $\\mathbf{w} \\cdot \\mathbf{x} + b < 0$ for all data points of the negative class (here arbitrarily chosen to be \"water\").\n",
        "\n",
        "One useful visualization of a classifier with 2D inputs are plots that show the **decision boundary**. This is a line (or multiple lines) that separate the 2D space such that on one side of the boundary the classifier predicts one class for all points, and on the other side of the boundary, the classifier predicts the other class for all points.\n",
        "\n",
        "<br/>\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        ">In the cell below, adjust the values of the weight vector $\\mathbf{w}$ and the bias term $b$. Keep adjusting them until you find a decision boundary that separates the two classes.\n",
        ">\n",
        "> Consider the folllowing:\n",
        ">* Choose a weight vector that is close to the prompt embeddings of the positive class (i.e., the points of the class \"food\"). This will result in a weight vector that has high similarity to the points of the \"food\" class. Remember from the previous course that two vectors with high similarity, have a positive dot product. So if you point the weight vector towards examples of the positive class, you are making it similar to examples of that class.\n",
        ">* Investigate what happens when the dot product between the weight vector and a data point is zero (or close to 0), that is, when $\\mathbf{w} \\cdot \\mathbf{x} + b \\approx 0$. Data points for which this is true are called **orthogonal points**.\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHowK1nnXrw5"
      },
      "outputs": [],
      "source": [
        "# Initial guess for weights and bias.\n",
        "weight_vector = jnp.array([-0.5, 0.45])  # Weight vector: [w1, w2].\n",
        "bias_term = 0.2  # Bias term.\n",
        "\n",
        "visualizations.plot_data_and_decision_boundary(\n",
        "    input_features, labels, weight_vector, bias_term, provide_feedback=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sj5Z3YM-cwU"
      },
      "source": [
        "### What did you observe?\n",
        "\n",
        "Note that the points of the positive class (\"food\") are all concentrated around $(1,1)$. Consequently, if you choose a weight vector that is similar to $(1,1)$, this vector will point in the direction of the positive class. This means that $\\mathbf{w} \\cdot \\mathbf{x} + b > 0$ for all points of the positive class, and $\\mathbf{w} \\cdot \\mathbf{x} + b < 0$ for all points of the negative class (\"water\").\n",
        "\n",
        "By setting the bias term to a value close to $0$, you further avoid orthogonal points, that is, points for which the dot product is $0$ or close to $0$. A decision boundary such as this is ideal, as it is as far away as possible from all points while perfectly separating the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QljJJ84WA8fE"
      },
      "source": [
        "## Summary\n",
        "In this lab, you explored how to separate two classes of token embeddings in 2D space using a single artificial neuron. You investigated how to reposition the decision boundary to separate the classes properly by adjusting the weight vector and bias. Through manual tuning, you gained an understanding of how dot products relate to classification.\n",
        "\n",
        "You observed that, to separate the classes well, the weight vector needs to be pointed in the direction of the \"positive\" class (\"food\" in this case). Additionally, you observed that when a data point lies exactly on the boundary, its dot product with the weight vector is zero. This means that the point is orthogonal to the boundary, indicating that the point is not classified into any class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "andYV2wsTRA0"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3TlK2CBR0I1"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B0dphCpR3l5"
      },
      "outputs": [],
      "source": [
        "# One setting that perfectly separates the data.\n",
        "weight_vector = jnp.array([1.0, 1.0])\n",
        "bias_term = 0  # Bias term.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "andYV2wsTRA0",
        "d3TlK2CBR0I1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
