{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DAsej7XSlYt"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myzQnLMOJ91"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C3-white-bg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE0jaJsaICiX"
      },
      "source": [
        "# Lab: Gradients\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_3/gdm_lab_3_7_gradients.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Automatically compute gradients to incrementally learn decision boundaries.\n",
        "\n",
        "25 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSYq3aNrdpnC"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will explore how you can use JAX to compute gradients automatically, a process known as **automatic differentiation** (or **autodiff** for short). You will use the automatically computed gradients to repeatedly make updates to the decision boundary of a binary classifier until it perfectly separates the two classes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Qf3eGhfl4H"
      },
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will:\n",
        "* Compute gradients with JAX.\n",
        "* Investigate how these gradients can be used to update the parameters of a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIhcVxx0foVo"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "For this activitiy, you will reuse the dataset from the earlier activities where each data point is a two-dimensional prompt embedding. The model's task is to  predict the next token (\"food\" or \"water\"). As a model, you will use a single-neuron neural network with a sigmoid activation function.\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Load the dataset of 2-dimensional embeddings.\n",
        "* Manually define the model computations and loss function using functions from JAX.\n",
        "* Automatically compute the gradients of this model.\n",
        "* Observe how the decision boundary shifts as you are making updates.\n",
        "\n",
        "All of these steps are described in detail in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNG_jg-39Zj"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in *cells* that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtgZxrpjm6j"
      },
      "source": [
        "Note that the order in which you run the cells matters. When you are working through a lab, make sure to always run all cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-d1hd7Xndke"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will primarily use functions from JAX for defining the model, computing the gradient, making parameter updates, and representing the data.\n",
        "\n",
        "Run the following cell to import all required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMquCBmpLgX0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "# Packages used.\n",
        "import jax # For automatically computing gradients.\n",
        "import jax.numpy as jnp # For defining the model and input matrices.\n",
        "from IPython.display import display # For displaying the dataset.\n",
        "import pandas as pd # For loading and displaying the dataset.\n",
        "from ai_foundations import visualizations # For creating plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hUezjKC0LaI"
      },
      "source": [
        "## Load the dataset\n",
        "\n",
        "Run the following cell to load and print the dataset. Note, this is the 2-class dataset that you used in one of the early labs in this course, not the 3-class dataset that you have been more recently using to learn about MLPs and hyperparameter tuning.\n",
        "\n",
        "Also note that for this activity, there is no validation or test set. This is because, the goal of this activity is not to build a model that generalizes best but rather to understand how the training process itself works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S3dRN2f0IuX"
      },
      "outputs": [],
      "source": [
        "# Load data using pandas.\n",
        "df = pd.read_csv(\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/food-water-dataset.csv\")\n",
        "\n",
        "# Extract features (Embedding_dim_1, Embedding_dim_2) and labels.\n",
        "# Input features (token embeddings).\n",
        "X_train = jnp.array(df[[\"Embedding_dim_1\", \"Embedding_dim_2\"]].values)\n",
        "labels = df[\"Label\"].values # Labels: \"food\" or \"water\".\n",
        "\n",
        "# Convert labels to numeric values for plotting (food = 1, water = 0).\n",
        "y_train = jnp.where(labels == \"food\", 1, 0)\n",
        "\n",
        "# Print and visualize the loaded data for verification.\n",
        "display(df.head(n=20))\n",
        "\n",
        "visualizations.plot_data_and_decision_boundary(X_train, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpyHfts40kGY"
      },
      "source": [
        "## Coding activity 1: Initial guess\n",
        "\n",
        "Recall, that the weight vector ($\\mathbf{w}$) determines the orientation of the decision boundary, and the bias term ($b$) determines how much the decision boundary is shifted.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        ">Change the values of $\\mathbf{w}$ and $b$ below\n",
        "and observe how the decision boundary shifts.\n",
        ">\n",
        ">Focus on understanding:\n",
        ">- How the weight vector ($\\mathbf{w}$) defines the slope of the decision boundary.\n",
        ">- How the bias term ($b$) defines the intercept of the decision boundary.\n",
        ">- What happens when the weight vector points towards the positive class or the negative class.\n",
        ">\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVnSUR-H03v5"
      },
      "outputs": [],
      "source": [
        "# Change the bias and weights here.\n",
        "b_initial_guess = jnp.array([0.5])\n",
        "w_initial_guess = jnp.array([-0.9, 0.9])\n",
        "\n",
        "# Plot the initial guess for the weight vector.\n",
        "visualizations.plot_data_and_decision_boundary(\n",
        "    X_train,\n",
        "    labels,\n",
        "    weight_vector=w_initial_guess,\n",
        "    bias_term=b_initial_guess,\n",
        "    title=\"Initial Guess for Weights\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9nqoS-F1CC1"
      },
      "source": [
        "## Coding activity 2: Implement the prediction function\n",
        "\n",
        "In order for JAX to compute the gradients for a model, it needs an implementation of the loss function. The loss function in return needs an implementation of the neural network that outputs predictions.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Complete the function `predict` below. This function should implement a single neuron with a sigmoid activation function using tools from `JAX`.\n",
        ">\n",
        "> As a reminder, a single neuron makes predictions for an input vector $\\mathbf{x}$ as follows:\n",
        ">\n",
        "> $$y = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)$$\n",
        ">\n",
        "> To compute the dot product between two vectors $\\mathbf{a}$ and $\\mathbf{b}$, you can use the `jnp.dot(a,b)` function. To compute $\\sigma(x)$, you can use the `jax.nn.sigmoid(x)` function.\n",
        ">\n",
        "> Note, that the input for the predict function is a matrix $X$ rather than a vector. This matrix includes the embedding of one prompt in each row. If you call `jnp.dot` on this matrix instead, it will automatically return a vector $\\mathbf{y}$ that contains all predictions for all examples represented by $X$. This alllows you to jointly compute the predictions for all examples in a dataset rather than computing them one-by-one. If you do this, however, you will have to change the order of arguments since you are now performing a matrix multiplication between a matrix $X$ of dimension `number_of_examples x embedding_dim` and a matrix $w$ of dimension `embedding_dim x 1`. Unlike for dot products, the order in which you multiply matrices matters and the second dimension of the first matrix and the first dimension of the second matrix have to be the same. Be sure to consider this when you implement the prediction function below.\n",
        "------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n43EYupF0chI"
      },
      "outputs": [],
      "source": [
        "def predict(w: jax.Array, b: jax.Array, X: jax.Array) -> jax.Array:\n",
        "    \"\"\"Computes predicted probabilities using a single neuron.\n",
        "\n",
        "    This function calculates the dot product of the input features and weights,\n",
        "    then applies the sigmoid activation function to produce predictions in the\n",
        "    range [0, 1].\n",
        "\n",
        "    Args:\n",
        "        w: The model's weight vector. Expected shape is (embedding_dim,).\n",
        "        b: The model's bias term. Expected shape is (1,).\n",
        "        X: The input data matrix where each row is a sample and each column\n",
        "           is an embedding dimension. Expected shape is\n",
        "           (number_of_examples, embedding_dim).\n",
        "\n",
        "    Returns:\n",
        "        An array of predicted probabilities, one for each sample. The shape\n",
        "        of the output array is (number_of_examples,).\n",
        "    \"\"\"\n",
        "\n",
        "    y = ... # Add your code here.\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLgLySpTDSDn"
      },
      "source": [
        "## Define the loss function\n",
        "\n",
        "Run the following cell to define the loss function. As you are training a binary classifier, use the binary cross-entropy loss, where $y_{\\mathrm{pred}}$ is the predicted probability for class 1, and $y_{\\mathrm{true}}$ is 1 if the true class is 1 (\"food\"), and 0 (\"water\") otherwise:\n",
        "\n",
        "$$\\mathrm{Loss}\\left(y_{\\mathrm{true}}, \\, y_{\\mathrm{pred}}\\right) = -\\left[y_{\\mathrm{true}} \\log \\left(y_{\\mathrm{pred}}\\right) + \\left(1-y_{\\mathrm{true}}\\right) \\log \\left(1-y_{\\mathrm{pred}}\\right)\\right]$$\n",
        "\n",
        "A common way to compute the loss across an entire dataset is to compute the average loss across all examples. This is done with the `jnp.mean` function below.\n",
        "\n",
        "The loss function below uses the current weights $\\mathbf{w}$ and the current bias term $b$ to compute the model predictions on the entire dataset using the `predict` function that you implemented above. This has the advantage of making the loss function directly depend on $\\mathbf{w}$ and $b$ and the automatic differentiation tools can then automatically compute the gradients for the loss function with respect to $\\mathbf{w}$ and $b$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17B8gWB1ERhh"
      },
      "outputs": [],
      "source": [
        "def loss_fn(\n",
        "    w: jax.Array, b: jax.Array, X: jax.Array, y_true: jax.Array\n",
        ") -> jax.Array:\n",
        "    \"\"\"Computes the binary cross-entropy loss.\n",
        "\n",
        "    This loss function is suitable for binary classification tasks. It\n",
        "    quantifies the difference between the true binary labels and the\n",
        "    probabilities predicted by the model. A small epsilon value is added for\n",
        "    numerical stability to avoid log(0).\n",
        "\n",
        "    Args:\n",
        "      w: The model's weight vector. Expected shape is (embedding_dim,).\n",
        "      b: The model's bias term. Expected shape is (1,).\n",
        "      X: The input data matrix.\n",
        "        Expected shape is (number_of_examples, embedding_dim).\n",
        "      y_true: The true binary labels (0 or 1).\n",
        "        Expected shape is (number_of_examples,).\n",
        "\n",
        "    Returns:\n",
        "      The mean binary cross-entropy loss as a scalar value.\n",
        "    \"\"\"\n",
        "    y_pred = predict(w, b, X)\n",
        "    # Add a small epsilon for numerical stability.\n",
        "    epsilon = 1e-8\n",
        "    return jnp.mean(\n",
        "        -(\n",
        "            y_true * jnp.log(y_pred + epsilon)\n",
        "            + (1 - y_true) * jnp.log(1 - y_pred + epsilon)\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVtHziEVFHZr"
      },
      "source": [
        "Run the following cell to compute the loss for a classifier using your manually set weights on the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3g3q-NXI1cGz"
      },
      "outputs": [],
      "source": [
        "# Compute the loss for the initial weights.\n",
        "print(\n",
        "    \"Loss for current weights:\"\n",
        "    f\" {loss_fn(w_initial_guess, b_initial_guess, X_train, y_train):.4f}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3LnDhJ_1jqT"
      },
      "source": [
        "## Compute the gradients and update the weights\n",
        "\n",
        "You will now perform one automatic training step. The cell below uses JAX's `grad` function to define a function that can automatically compute the gradient of the loss function on a dataset.\n",
        "\n",
        "This gradient tells you how to adjust the weights and the bias terms to reduce the loss.\n",
        "\n",
        "For the update, you will compute the updated weight vector as\n",
        "\n",
        "```python\n",
        "w_updated = w_current - learning_rate * gradient_w  \n",
        "```\n",
        "\n",
        "and the updated bias term as:\n",
        "\n",
        "```python\n",
        "b_updated = b_current - learning_rate * gradient_b  \n",
        "```\n",
        "\n",
        "That means you subtract the gradient from the original weight vector. In order to make small, incremental updates, the gradient is multiplied with the `learning_rate` hyperparameter. This is set to a value smaller than 1.0, in fact, usually much smaller than 1.0.\n",
        "(The reason for making incremental updates will be explained in later activities.)\n",
        "\n",
        "Run the following cell to perform one automatic updating step.\n",
        "\n",
        "Focus on understanding:\n",
        "- How the gradient points in the direction of the **steepest ascent** (you subtract the gradient to go in the direction of the steepest descent).\n",
        "- How applying the gradient moves the weights towards better classifying the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCs9qQVU1jez"
      },
      "outputs": [],
      "source": [
        "# Reset the initial guess.\n",
        "b_initial_guess = jnp.array([0.5])\n",
        "w_initial_guess = jnp.array([-0.9, 0.9])\n",
        "\n",
        "# Define the gradient function that computes the gradient of the loss function.\n",
        "# `argnums=[0,1]` tells the automatic differentation method that it should\n",
        "# compute the gradient with respect to the first and the second argument of the\n",
        "# loss function, which are the weights and the bias term.\n",
        "grad = jax.grad(loss_fn, argnums=[0, 1])\n",
        "\n",
        "# Apply one gradient step.\n",
        "\n",
        "# Step size for updating the weights.\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Compute the gradient.\n",
        "gradient = grad(w_initial_guess, b_initial_guess, X_train, y_train)\n",
        "print(f\"Gradients: gradient_w = {gradient[0]}, gradient_b = {gradient[1]}\")\n",
        "print(\n",
        "    f\"Loss before update:\"\n",
        "    f\" {loss_fn(w_initial_guess, b_initial_guess, X_train, y_train):.4f}\"\n",
        ")\n",
        "\n",
        "# `gradient` is a tuple that contains the gradient with respect to the weight\n",
        "# vector and the gradient with respect to the bias term.\n",
        "gradient_w, gradient_b = gradient\n",
        "\n",
        "# Update the weights and bias term.\n",
        "w_updated = w_initial_guess - learning_rate * gradient_w\n",
        "b_updated = b_initial_guess - learning_rate * gradient_b\n",
        "\n",
        "print(\n",
        "    f\"Loss after update:\"\n",
        "    f\" {loss_fn(w_updated, b_updated, X_train, y_train):.4f}\"\n",
        ")\n",
        "\n",
        "# Plot the initial guess for the weight vector.\n",
        "visualizations.plot_data_and_decision_boundary(\n",
        "    X_train,\n",
        "    labels,\n",
        "    weight_vector=w_updated,\n",
        "    bias_term=b_updated,\n",
        "    title=\"Decision Boundary After One Gradient Step\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39umNoQ91wpk"
      },
      "source": [
        "### Gradient descent\n",
        "\n",
        "The gradient descent algorithm works by repeatedly performing updates to the weight vector and the bias term using the gradient update formulas from above:\n",
        "\n",
        "```python\n",
        "w = w - learning_rate * gradient_w  \n",
        "```\n",
        "\n",
        "and\n",
        "\n",
        "```python\n",
        "b = b - learning_rate * gradient_b  \n",
        "```\n",
        "\n",
        "Here, `w` and `b` store the current value of the weights and the bias term, which need to be initialized at the beginning of the training process. In practice, this intialization is usually done randomly but here, it is initialized with the values in `w_initial_guess` and `b_initial_guess`.\n",
        "\n",
        "These updates are performed as part of the training loop. Each iteration of this training loop corresponds to one epoch, so the number of times this loop gets executed is the number of epochs. With each iteration, the training loss decreases and the model becomes better at making predictions on examples in the training data.\n",
        "\n",
        "Run the following cell to repeatedly apply the gradient updates to see how the weights evolve over time.\n",
        "\n",
        "Focus on observing:\n",
        "- How the decision boundary shifts closer to separating the two classes.\n",
        "- How the weight vector gradually aligns with the positive class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x3B7XO41ehi"
      },
      "outputs": [],
      "source": [
        "# Intialize the weight vector.\n",
        "w = w_initial_guess\n",
        "b = b_initial_guess\n",
        "learning_rate = 0.5\n",
        "\n",
        "for step in range(10):  # Perform 10 update steps.\n",
        "\n",
        "    # Compute the gradients.\n",
        "    gradient = grad(w, b, X_train, y_train)\n",
        "    gradient_w, gradient_b = gradient\n",
        "\n",
        "    # Update the weights and bias term.\n",
        "    w = w - learning_rate * gradient_w\n",
        "    b = b - learning_rate * gradient_b\n",
        "\n",
        "    print(f\"Step {step + 1} loss: {loss_fn(w, b, X_train, y_train):.4f}\")\n",
        "\n",
        "    visualizations.plot_data_and_decision_boundary(\n",
        "        X_train,\n",
        "        labels,\n",
        "        weight_vector=w,\n",
        "        bias_term=b,\n",
        "        title=f\"Step {step + 1}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dHYn-yCJ-_W"
      },
      "source": [
        "You have now seen what happens during model training. While this process is much more complex for models with multiple layers with many neurons, the principles are exactly the same. On each iteration, the gradient of the loss function is computed and this gradient is used to update the parameters. If this step is repeated many times for all examples in the training data, the model can learn sophisticated patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fjc20pJjBTc"
      },
      "source": [
        "## More complex machine learning problems\n",
        "\n",
        "------\n",
        "> **ðŸ’­ Reflection:**\n",
        ">\n",
        "> Run the following cell to plot the spiral dataset that contains datapoints separated into four classes. Then reflect upon the models and methods you have learned in this and previous activities.\n",
        ">\n",
        ">* How would you extend the logistic regression model to support more than two classes (e.g., four in this case)?\n",
        ">\n",
        ">* Could a classifier with a single activation function such as a logistic regression model properly separate this data? Why or why not?\n",
        ">\n",
        "-----\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgLOcGGyjJXE"
      },
      "outputs": [],
      "source": [
        "visualizations.plot_spiral_data(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/spiral-dataset.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2AMLpzsgr5L"
      },
      "source": [
        "## Summary\n",
        "In this activitiy, you learned how you can automatically compute **gradients** using the `JAX` package. You then used these gradients to run **gradient descent** and learn model parameters that created a good decision boundary for the dataset. This demonstrated the training process of any neural network model, regardless of its complexity.\n",
        "\n",
        "In the following activities, you will learn more about how the gradient updates and how the updates are performed when you are training the model on a much bigger dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ5a_ybsMIcM"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPqmLeeR-wOq"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_gvBMpg-yCP"
      },
      "outputs": [],
      "source": [
        "# One potential solution.\n",
        "b_initial_guess = jnp.array([0.0])\n",
        "w_initial_guess = jnp.array([1.0, 1.0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAOkfB5DDbV1"
      },
      "source": [
        "### Coding Activity 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac9r8goBDdOr"
      },
      "outputs": [],
      "source": [
        "def predict(w: jax.Array, b: jax.Array, X: jax.Array) -> jax.Array:\n",
        "    \"\"\"Computes predicted probabilities using a single neuron.\n",
        "\n",
        "    This function calculates the dot product of the input features and weights,\n",
        "    then applies the sigmoid activation function to produce predictions in the\n",
        "    range [0, 1].\n",
        "\n",
        "    Args:\n",
        "        w: The model's weight vector. Expected shape is (embedding_dim,).\n",
        "        b: The model's bias term. Expected shape is (1,).\n",
        "        X: The input data matrix where each row is a sample and each column\n",
        "           is a feature. Expected shape is (number_of_examples, embedding_dim).\n",
        "\n",
        "    Returns:\n",
        "        An array of predicted probabilities, one for each sample. The shape\n",
        "        of the output array is (number_of_examples,).\n",
        "    \"\"\"\n",
        "\n",
        "    y = jax.nn.sigmoid(jnp.dot(X, w) + b)\n",
        "\n",
        "    return y"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "XQ5a_ybsMIcM",
        "KPqmLeeR-wOq",
        "OAOkfB5DDbV1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
