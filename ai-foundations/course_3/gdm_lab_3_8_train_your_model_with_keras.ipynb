{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hxkhChw90-o"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myzQnLMOJ91"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C3-white-bg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE0jaJsaICiX"
      },
      "source": [
        "# Lab: Train Your Model with Keras\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_3/gdm_lab_3_8_train_your_model_with_keras.ipynb'\n",
        "target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Train a neural network model using optimizers implemented in Keras.\n",
        "\n",
        "25 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSYq3aNrdpnC"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In the previous article, you have learned how stochastic gradient descent (SGD) can be used to train a neural network model. In this lab, you will put that knowledge into practice and explore how you can use the SGD-based optimizer **Adam** to train Keras models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Qf3eGhfl4H"
      },
      "source": [
        "### What you will learn:\n",
        "\n",
        "By the end of this lab, you will understand:\n",
        "\n",
        "* How you can train any Keras model, independent of its architecture.\n",
        "* The role of the optimizer and loss function and how they can be combined to train a model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIhcVxx0foVo"
      },
      "source": [
        "### Tasks\n",
        "\n",
        "As in the previous labs, you will work with a dataset of prompt embeddings. The goal is to build a classifier that predicts the next word (\"food\" or \"water\") from the prompt embedding.\n",
        "\n",
        "**In this lab, you will**:\n",
        "* Load the dataset.\n",
        "* Define a two-layer neural network model using Keras.\n",
        "* Train the model using Keras implementations of the loss function, the Adam optimizer, and the training loop.\n",
        "\n",
        "All of these steps are described in detail in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNG_jg-39Zj"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in *cells* that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtgZxrpjm6j"
      },
      "source": [
        "Note that the order in which you run the cells matters. When you are working through a lab, make sure to always run all cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-d1hd7Xndke"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will primarily use the [`keras`](https://keras.io/) package for defining and training neural network models.\n",
        "\n",
        "Run the following cell to import all required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVX5j7HQO91i"
      },
      "outputs": [],
      "source": [
        "import os # For adjusting Keras settings.\n",
        "os.environ['KERAS_BACKEND'] = 'jax' # Set a parameter for Keras.\n",
        "\n",
        "import jax.numpy as jnp # For defining and working with vectors and matrices.\n",
        "import keras # For defining and training neural nework models.\n",
        "import pandas as pd # For displaying and loading data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCtlBPEFWhCZ"
      },
      "source": [
        "## Load the data\n",
        "\n",
        "Run the following cell to download the dataset with 2-dimensional sentence embeddings. As in previous labs, the goal is to predict the next word from the words \"food\" (numeric label 1) and \"water\" (numeric label 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-wSlCdUPmI4"
      },
      "outputs": [],
      "source": [
        "# Load data using pandas.\n",
        "df = pd.read_csv(\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/food-water-dataset.csv\")\n",
        "\n",
        "# Extract embeddings and labels.\n",
        "X_train = jnp.array(df[[\"Embedding_dim_1\", \"Embedding_dim_2\"]].values)\n",
        "labels = df[\"Label\"].values # Labels: \"food\" or \"water\".\n",
        "# Convert labels to numeric values for training the model (food = 1, water = 0).\n",
        "y_train = jnp.where(labels == \"food\", 1, 0)\n",
        "df[\"Numeric label\"] = y_train\n",
        "\n",
        "# Print the loaded data for verification.\n",
        "df.head(n=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G_N2jfdP1r-"
      },
      "source": [
        "## Define the neural network model\n",
        "\n",
        "The following cell implements a function `build_neural_network` for defining a two-layer neural network using Keras.\n",
        "\n",
        "The operations of the hidden layer are defined in\n",
        "```python\n",
        "operations.append(keras.layers.Dense(hidden_dim, activation=\"relu\"))\n",
        "```\n",
        "\n",
        "and the operations of the output layer are defined in\n",
        "```python\n",
        "operations.append(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "```\n",
        "\n",
        "Reflect upon why this model is using a sigmoid activation function as the output layer? Could this be replaced with a SoftMax? When would this be useful?\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **â„¹ï¸ Info: Combining layer operations**\n",
        ">\n",
        ">Note that for defining both layers, this code is combining two operations in one call here. When you implemented the MLP in one of the previous labs, you defined the computation of the dot product and the application of the activation function separately. However, as you rarely want to define a hidden layer without an activation function, Keras allows you to combine these two steps by passing the argument `activation` to the intialization of the `Dense` layer. You can find a list of available activation functions in the [Keras documentation](https://keras.io/api/layers/activations/).\n",
        ">\n",
        "------\n",
        "\n",
        "<br />\n",
        "\n",
        "Inspect both the documentation of the `build_neural_network` function and its implementation to understand how it defines a neural network. Then run the following cell to define it so that you can use it later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOE3ealKkUgd"
      },
      "outputs": [],
      "source": [
        "def build_neural_network(hidden_dim: int = 10) -> keras.Model:\n",
        "  \"\"\"\n",
        "  A function that intializes a two-layer neural network for binary\n",
        "  classification, implemented in Keras. The hidden layer uses a ReLU activation\n",
        "  function, and the ouput layer uses a sigmoid activation function.\n",
        "\n",
        "  Args:\n",
        "    hidden_dim: The dimension of the hidden layer.\n",
        "\n",
        "  Returns:\n",
        "    A keras.Model instance that implements the logistic regression model.\n",
        "  \"\"\"\n",
        "\n",
        "  operations = []\n",
        "\n",
        "  # Add the operations for a hidden layer with a ReLU activation function.\n",
        "  operations.append(keras.layers.Dense(hidden_dim, activation=\"relu\"))\n",
        "\n",
        "  # Add the operations for an output layer with a sigmoid activation function.\n",
        "  operations.append(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "  # Construct a model such that inputs are passed sequentially through every\n",
        "  # layer.\n",
        "  model = keras.Sequential(operations)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zb2-we4Z3lI"
      },
      "source": [
        "------\n",
        "> **â„¹ï¸ Info: The ingredients for training a model**\n",
        ">\n",
        ">Most deep learning frameworks, including Keras, use a combination of three components to train a neural network model:\n",
        ">* The **loss function**: As you have already seen, the loss is a function of the current model weights, the current model predictions and the target labels in the training data. This is the function that you try to optimize during training. A lower value of this function means that the model is better at making predictions on examples in the training data.\n",
        ">* The **optimizer**: This component is responsible for updating the parameters (weights) of the model such that the loss decreases. In optimizers that you would use for training neural networks, the updating is based on the gradient of the loss function with respect to the training examples, as you observed in the discussion of the SGD algorithm. All optimizers have to be initialized with the `learning_rate` parameter. This parameter defines how big each update step should be.\n",
        ">* The **model**: This component defines which computations are needed to process the input. When you define a model using a deep learning framework such as Keras, this also automatically defines the necessary parameters for each layer. For example, when you define a layer using `Dense`, this also initializes the weights and the bias term for that layer.\n",
        ">\n",
        "------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIqjJvU8eZK6"
      },
      "source": [
        "## Coding activity 1: Define a loss function\n",
        "\n",
        "Before you can train a model, you have to define both the loss function and the optimizer. In almost all cases, you can use existing implementations for both of these components.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtlMHOfIZSr6"
      },
      "source": [
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Your first task is to define the loss function. Remember that you are building a **binary classifier** that predicts the probability of the next word being \"food\" (class 1). As you have seen in the previous lab and articles, for such classifiers, you will use a binary cross-entropy loss. In Keras, loss functions are defined in the `keras.losses` module and the binary cross-entropy loss can be initialized using the following class:\n",
        ">\n",
        ">```python\n",
        ">keras.losses.BinaryCrossentropy()\n",
        ">```\n",
        ">\n",
        ">Define the binary cross-entropy loss in the cell below.\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkrrUSt8Y8LB"
      },
      "outputs": [],
      "source": [
        "# Define the loss function.\n",
        "loss_fn = # Add your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c0sJVnGaUcu"
      },
      "source": [
        "------\n",
        "> **â„¹ï¸ Info: Other loss functions**\n",
        ">\n",
        ">If you were doing a multi-class classification task (i.e., a classifier that chooses between more than two classes), you would use a `keras.losses.CategoricalCrossentropy` or a `keras.losses.SparseCategoricalCrossentropy` loss function. You can find a list of all loss functions implemented in Keras in the [Keras documentation](https://keras.io/api/losses/).\n",
        ">\n",
        "------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi9-zEgmgyA5"
      },
      "source": [
        "## Coding activity 2: Define the optimizer\n",
        "\n",
        "As an optimizer, you will almost always use the Adam optimizer. This optimizer implements a more sophisticated version of the gradient update step than the regular SGD algorithm by adapting the step size depending on the shape of the loss function. In practice, this optimizer works very well for training neural networks.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Define the optimizer. The optimizer will compute the gradients and use them to update the parameters on each batch.\n",
        ">\n",
        ">In Keras, you can define the Adam optimizer as follows:\n",
        ">```python\n",
        ">keras.optimizers.Adam(learning_rate=<LEARNING RATE>)\n",
        ">```\n",
        "> If you want to apply weight decay, you can set its strength by adding the optional `weight_decay` parameter.\n",
        ">\n",
        "> Define this optimizer in the cell below. Use a learning rate of 0.01.\n",
        "------\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LTh8Wpol4ij"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer.\n",
        "optimizer = # Add your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtvfVmHhilsv"
      },
      "source": [
        "## Putting it all together: The `compile` method\n",
        "\n",
        "Finally, once you have defined the loss function, and the optimizer, you can put all of these components together in preparation for training.\n",
        "\n",
        "First, you need to define your model. This should be an instance of the `keras.Model` class. In this activity, you will use the `build_neural_network` function from above to define an MLP.\n",
        "\n",
        "Then, to put everything together, you can use the `compile` method of the model. This method attaches the loss function and the optimizer to the model. You can also specify optional metrics, such as the accuracy. If you specify metrics, the result of applying the metric after each epoch will be printed as part of the training log.\n",
        "\n",
        "Run the following cell to define the model and combine it with the loss function, and the optimizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTE82aXOoXbX"
      },
      "outputs": [],
      "source": [
        "# Set a random seed for reproducibility.\n",
        "keras.utils.set_random_seed(126)\n",
        "\n",
        "# Define a model.\n",
        "model = build_neural_network(hidden_dim = 10)\n",
        "\n",
        "# Attach the loss function, the optimizer and metrics.\n",
        "model.compile(loss=loss_fn, optimizer=optimizer, metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzhVKP58bdY_"
      },
      "source": [
        "## Coding activity 3: Train the model\n",
        "\n",
        "------\n",
        "> **ðŸ’» Your task:**\n",
        ">\n",
        "> Train the model.\n",
        ">\n",
        "> To train the model, you can use the `model.fit()` method. This method takes the following arguments:\n",
        ">* `x`: The input of the training data (a JAX array, `X_train` in this case).\n",
        ">* `y`: the target values in the training data (a JAX array, `y_train` is this case).\n",
        ">* `epochs`: The number of epochs. This specifies how many times the model loops through all training examples.\n",
        ">* `batch_size` (optional): This specifies how many examples there should be in one mini-batch.\n",
        ">* `validation_data` (optional): A tuple `(X_val, y_val)` to compute the validation loss and accuracy after each epoch.\n",
        ">* `callbacks` (optional): A list of functions that should be executed at the end of each epoch. One useful function to include in this list is `keras.callbacks.EarlyStopping()` to implement early stopping.\n",
        ">\n",
        ">The function returns the training history that contains the loss and the accuracy after each epoch of training.\n",
        ">\n",
        ">In the following cell, implement training with the `fit` method. Train the model for 100 epochs with a mini-batch size of 8.\n",
        "\n",
        "------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spXTT66_bpGR"
      },
      "outputs": [],
      "source": [
        "# Use the model.fit() method to train your model.\n",
        "history = # Add your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFK7cSfJS76"
      },
      "source": [
        "Congratulations! You now know all the steps involved in training neural network models with Keras. In the future, come back to this notebook if you need to train a Keras model to revise the steps.\n",
        "\n",
        "You can also experiment with changing the model, or changing the parameters of the optimizer, or the training procedure to observe how the training process changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJI3gjw0i4jd"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this activity, you trained a simple neural network using the Adam optimizer. You learned how the loss function, the optimizer and the model are combined in Keras to allow you to train any neural network model. You also learned where you can set hyperparameters such as the learning rate, the weight decay strength, or the number of epochs.\n",
        "\n",
        "In future courses, you will apply this knowledge to train and fine-tune language models. While those models will be more complicated than the MLP that you have been working with here, all the same principles will apply."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlX5ZMjEZmVs"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqQQo-eiZpJ5"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egZf5tTNZrAZ"
      },
      "outputs": [],
      "source": [
        "# Copy this into the cell above.\n",
        "loss_fn = keras.losses.BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLCVsjdBen9b"
      },
      "source": [
        "### Coding Activity 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLXDYzGIedSF"
      },
      "outputs": [],
      "source": [
        "# Copy this into the cell above.\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0gO-6dRelSE"
      },
      "source": [
        "### Coding Activity 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHrVbz85ec9A"
      },
      "outputs": [],
      "source": [
        "# Copy this into the cell above.\n",
        "history = model.fit(x=X_train, y=y_train, epochs=10, batch_size=8)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "NDWsJUGcf4Ru",
        "KqQQo-eiZpJ5",
        "TLCVsjdBen9b",
        "c0gO-6dRelSE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
