{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADVH0-Ys0sXp"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_JDPTnF0v9V"
      },
      "source": [
        "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C4-white-bg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlnvaHlv0zmj"
      },
      "source": [
        "# Lab: Implement the Attention Equation\n",
        "\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_4/gdm_lab_4_2_implement_attention_equation.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Compute the attention weight vector for all layers of a transformer model.\n",
        "\n",
        "15 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7fZa-AxFuRl"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will implement the attention equation yourself. You will use the learned projection matrices $W_Q$, $W_K$, $W_V$ from the Gemma-1B model and use these three matrices to compute the attention weights and the new contextual embedding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEGtVB_v3wfg"
      },
      "source": [
        "\n",
        "### What you will learn\n",
        "\n",
        "By the end of this lab, you will:\n",
        "\n",
        "* Understand how the attention weights are computed from query, key, and value matrices.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "* Load the pre-trained **Gemma-1B** model and use this model to compute query, key, and value matrices.\n",
        "* Implement the attention equation.\n",
        "* Visualize and compare your attention weights with those of the reference implementation.\n",
        "\n",
        "If you are able to, we recommend running the code in this lab on **a Colab instance with a GPU**. See the section \"How to use Google Colaboratory (Colab)\" below for instructions on how to do this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgKQCNqCBFS9"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fUG2Cn1BK1n"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over the cell and click on the `run` button to its left. The run button is the circle with the triangle (‚ñ∂). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or ‚åò+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC288-HfBL_1"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zNCFlnfBOuX"
      },
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime ‚Üí Run before__  from the menu above (or use the keyboard combination Ctrl/‚åò + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv4zdC_5vFEi"
      },
      "source": [
        "### Using Colab with a GPU\n",
        "\n",
        "A **GPU** is a special type of hardware that can significantly speed up some types of computations of machine learning models. Several of the activities in this lab will also run a lot faster if you run them on a GPU.\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click on **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware Accelerator**, select **GPU** (usually listed as `T4 GPU`).\n",
        "5.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaIF4eeW71gw"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will mainly work with the JAX package to modify vectors and matrices. You will also use functions from the custom `ai_foundations` package to load Gemma and extract its parameters and to generate visualizations.\n",
        "\n",
        "Run the following cell to import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI8ehGul6vsX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install orbax-checkpoint==0.11.21\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import jax # For working with vectors and matrices.\n",
        "import jax.numpy as jnp # For working with vectors and matrices.\n",
        "from ai_foundations import generation # For prompting the Gemma model.\n",
        "from ai_foundations import visualizations # For visualizing attention weights.\n",
        "from ai_foundations import attention # For working with Q,K,V matrices.\n",
        "# For providing feedback on your implementation.\n",
        "from ai_foundations.feedback.course_4 import attention as attention_feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CN0Ud9as1hZ6"
      },
      "source": [
        "## Load the model and visualize attention weights\n",
        "\n",
        "As mentioned, you will use the parameters from the Gemma-1B model to compute the attention weights.\n",
        "\n",
        "Run the following cell to load the Gemma-1B parameters. This also initializes a cache that speeds up the visualizations if you only adjust the layer parameter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1Bkw4zo1frt"
      },
      "outputs": [],
      "source": [
        "# Initialize caches for prompts.\n",
        "previous_prompt = None\n",
        "previous_prompt2 = None\n",
        "\n",
        "# Load special version of the Gemma-1B model that provides access to attention\n",
        "# weights and QKV matrices.\n",
        "print(\"Loading Gemma-1B...\")\n",
        "model = generation.load_gemma(\"Gemma-1B-AttentionWeight\")\n",
        "print(\"Loaded Gemma-1B.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdB7VKCChgjQ"
      },
      "source": [
        "The following cell allows you to visualize the Gemma attention weights, as in the previous lab. You can use this cell as a reference and compare your own implementation to the output of the reference implementation to make sure that you have implemented everything accurately.\n",
        "\n",
        "This visualization also includes an additional `show_all_weights` option. If you select this option, then the visualization will include attention weights for all target tokens, not just for the final token.\n",
        "\n",
        "Make sure to run this cell for at least one prompt as it also extracts the query, key, and value matrices that you will use in later exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2mJNICDcxgWH"
      },
      "outputs": [],
      "source": [
        "# @title Visualize attention weights (reference implementation)\n",
        "layer = 19  # @param {type:\"slider\", min: 0, max: 25}\n",
        "\n",
        "prompt = \"Jide was hungry so she went looking for\"  # @param {type: \"string\"}\n",
        "# @markdown Check the following box to display the attention weights for all tokens, not just for the generated one:\n",
        "show_all_weights = True  # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "if prompt != previous_prompt:\n",
        "  (\n",
        "      output_text,\n",
        "      _,\n",
        "      tokenizer,\n",
        "      attention_weights,\n",
        "      _,\n",
        "      qkv_dict,\n",
        "  ) = generation.prompt_attention_transformer_model(\n",
        "      prompt, model, sampling_mode=\"greedy\"\n",
        "  )\n",
        "  tokens = [tokenizer.tokens[t] for t in tokenizer.encode(output_text)]\n",
        "  previous_prompt = prompt\n",
        "\n",
        "print(f\"Generated text: {output_text}\")\n",
        "\n",
        "visualizations.visualize_attention(\n",
        "    tokens,\n",
        "    attention_weights[f\"layer_{layer}\"],\n",
        "    layer,\n",
        "    min_line_thickness=0,\n",
        "    max_line_thickness=5,\n",
        "    show_all_weights=show_all_weights,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqmFF9IY-lgk"
      },
      "source": [
        "## Coding Activity 1: Attention equation\n",
        "\n",
        "------\n",
        "> **üíª Your task:**\n",
        ">\n",
        ">Implement the attention mechanism for a specific layer in the following cell. To simplify things, the cell already makes a call to `get_qkv_matrices` which contains the query, key, and value projections for every token in the sentence.\n",
        ">\n",
        ">1. Inspect the query, key, and value projection matrices. What is their dimension?\n",
        ">\n",
        ">2. Compute $d_k$, the dimension of the key vectors which you need to compute the normalization factor. You can use [`.shape`](https://docs.jax.dev/en/latest/_autosummary/jax.Array.shape.html) for this.\n",
        ">\n",
        "> 3. Compute the logits by performing a matrix multiplication between the query projections and the key projections and normalizing them by the square root of $d_k$:\n",
        ">\n",
        ">    $$\\mbox{logits} = \\frac{QK^T}{\\sqrt{d_k}}$$\n",
        ">\n",
        ">    Instead of `jnp.matmul`, you can also use the [`@` operator](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.matmul.html) to multiply two matrices. For example, `A @ B` computes the matrix product between matrices `A` and `B`. To compute the square root, use the function `jnp.sqrt`. Finally, remember that you can transpose a matrix `A` by calling [`A.T`](https://docs.jax.dev/en/latest/_autosummary/jax.Array.T.html).\n",
        ">\n",
        "> 4. Compute the attention weights `alpha` by applying the SoftMax function to the logits. You can either implement your own SoftMax function or use [`jax.nn.softmax`](https://docs.jax.dev/en/latest/_autosummary/jax.nn.softmax.html):\n",
        ">\n",
        "> $$\\alpha = \\mbox{SoftMax}(\\mbox{logits})$$\n",
        ">\n",
        ">5. Compute the output of the attention mechanism by multiplying the attention weights $\\alpha$ with the value projections:\n",
        ">$$Y = \\alpha V$$\n",
        ">\n",
        ">Once you have implemented the computations by filling in the missing parts of the next cell, run the test cell and the \"Visualize attention weights (your implementation)\" cell to see the attention weights.\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgNJxOXrTJjY"
      },
      "outputs": [],
      "source": [
        "def compute_attention(\n",
        "    qkv_dict: dict[str, dict[str, jax.Array]], layer: int\n",
        ") -> tuple[jax.Array, jax.Array, jax.Array]:\n",
        "  \"\"\"Computes the attention weights for a layer.\n",
        "\n",
        "  Args:\n",
        "    qkv_dict: A dictionary containing the raw query, key, and value\n",
        "      projections for all layers. The keys are strings identifying the layers\n",
        "      and matrix types, and values are the corresponding JAX arrays.\n",
        "    layer: The specific layer for which to compute the attention weights.\n",
        "\n",
        "  Returns:\n",
        "    Y: The output of the attention layer. Shape: (n_tokens, embedding_dim).\n",
        "    alpha: The attention weights. Shape: (n_tokens, n_tokens).\n",
        "    logits: The raw logits used to compute the attention weights.\n",
        "      Shape: (n_tokens, n_tokens).\n",
        "  \"\"\"\n",
        "  # Extract the query, key, and value projection matrices from `qkv_dict`.\n",
        "  query_proj, key_proj, value_proj = attention.get_qkv_matrices(qkv_dict, layer)\n",
        "\n",
        "  logits = ...  # Compute the logits here.\n",
        "\n",
        "  alpha = ...  # Compute the attention vector here.\n",
        "\n",
        "  Y = ...  # Compute the output here.\n",
        "\n",
        "  return Y, alpha, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hAuitR6B6FPK"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to test your implementation\n",
        "attention_feedback.test_compute_attention(compute_attention, qkv_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_wtvwGGMqKf"
      },
      "source": [
        "### Visualize your attention weights\n",
        "\n",
        "The following cell uses your implementation of the attention weights and visualizes the output of the `compute_attention` function. Enter the same prompt as you entered before and compare the visualization of your attention weights to the original ones that were extracted from Gemma.\n",
        "\n",
        "For the same layer and the same prompt, both visualizations should be identical. If they are not, make sure to check your original implementation and try again.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-Ol7vD-hMoqs"
      },
      "outputs": [],
      "source": [
        "# @title Visualize attention weights (your implementation)\n",
        "layer = 19  # @param {type:\"slider\", min: 0, max: 25}\n",
        "\n",
        "prompt = \"Jide was hungry so she went looking for\"  # @param {type: 'string'}\n",
        "# @markdown Check the following box to display the attention weights for all tokens, not just for the generated one:\n",
        "show_all_weights = True  # @param {type:\"boolean\", label: \"sfdsf\"}\n",
        "\n",
        "model_name = \"Gemma-1B\"\n",
        "\n",
        "if prompt != previous_prompt2:\n",
        "  (\n",
        "      output_text2,\n",
        "      _,\n",
        "      tokenizer,\n",
        "      attention_weights2,\n",
        "      attention_mask2,\n",
        "      qkv_dict2,\n",
        "  ) = generation.prompt_attention_transformer_model(\n",
        "      prompt, model, sampling_mode=\"greedy\"\n",
        "  )\n",
        "  tokens2 = [tokenizer.tokens[t] for t in tokenizer.encode(output_text2)]\n",
        "  previous_prompt2 = prompt\n",
        "\n",
        "print(f\"Generated text: {output_text2}\")\n",
        "\n",
        "\n",
        "attn_out, attn_weights, attn_logits = compute_attention(qkv_dict2, layer)\n",
        "# Post-process attention weights to apply masking (to be covered in subsequent\n",
        "# activities), so that the attention weights are idential to the ones computed\n",
        "# by Gemma.\n",
        "attn_weights_post_processed = attention.post_process_attention(\n",
        "    attn_logits, attention_mask2\n",
        ")\n",
        "\n",
        "visualizations.visualize_attention(\n",
        "    tokens2,\n",
        "    attn_weights_post_processed,\n",
        "    layer,\n",
        "    min_line_thickness=0,\n",
        "    max_line_thickness=5,\n",
        "    show_all_weights=show_all_weights,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTfEWeyCnAum"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you implemented the computation of the attention mechanism of a transformer model. You used the key, query, and value projections from the Gemma model and combined them to compute the attention weights and the output of the attention mechanism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKkOGFZ1fEGy"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-HJZFuxf5Nz"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-3XFtoGf76e"
      },
      "outputs": [],
      "source": [
        "# Complete implementation of compute_attention.\n",
        "def compute_attention(\n",
        "    qkv_dict: dict[str, dict[str, jax.Array]], layer: int\n",
        ") -> tuple[jax.Array, jax.Array, jax.Array]:\n",
        "  \"\"\"Computes the attention weights for a layer.\n",
        "\n",
        "  Args:\n",
        "    qkv_dict: A dictionary containing the raw query, key, and value\n",
        "      projections for all layers. The keys are strings identifying the layers\n",
        "      and matrix types, and values are the corresponding JAX arrays.\n",
        "    layer: The specific layer for which to compute the attention weights.\n",
        "\n",
        "  Returns:\n",
        "    Y: The output of the attention layer.\n",
        "    alpha: The attention weights.\n",
        "    logits: The raw logits used to compute the attention weights.\n",
        "  \"\"\"\n",
        "  # Extract the query, key, and value projection matrices from `qkv_dict`.\n",
        "  query_proj, key_proj, value_proj = attention.get_qkv_matrices(qkv_dict, layer)\n",
        "\n",
        "  # Extract d_k.\n",
        "  _, dim_key = key_proj.shape\n",
        "\n",
        "  # Compute the logits = QK.T/sqrt(d_k)\n",
        "  logits = (query_proj @ key_proj.T) / jnp.sqrt(dim_key)\n",
        "\n",
        "  # Compute the attention weights ùõº.\n",
        "  alpha = jax.nn.softmax(logits)\n",
        "\n",
        "  # Compute the output Y = ùõºV.\n",
        "  Y = alpha @ value_proj\n",
        "\n",
        "  return Y, alpha, logits\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kgKQCNqCBFS9",
        "DKkOGFZ1fEGy",
        "_-HJZFuxf5Nz"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
