{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyJ3XihfpSws"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG27-2YhpUSD"
      },
      "source": [
        "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C2-white-bg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tkkZg3bYxQG"
      },
      "source": [
        "# Lab: Train an SLM with Your BPE Tokenizer\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_2/gdm_lab_2_6_train_an_slm_with_your_bpe_tokenizer.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Train your byte pair encoding tokenizer on the Africa Galore dataset and use it to train a transformer-based language model.\n",
        "\n",
        "25 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZH0ucIopZ6u"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will apply the skills you have learned in this course and 01 Build Your Own Small Language Model. You will again train a small language model (SLM) on the Africa Galore dataset. This time, however, you will first learn the **merges** for a **byte pair encoding (BPE) subword tokenizer** and then use this tokenizer to prepare your data for training the model. Finally, you will use the **t-SNE dimensionality reduction** technique from the previous lab to visualize some of the token embeddings that your model has learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvPRdN2g6WiL"
      },
      "source": [
        "### What you will learn\n",
        "\n",
        "By the end of this lab, you will understand:\n",
        "\n",
        "* How you can apply the BPE algorithm to learn merges from a text dataset.\n",
        "* How this tokenizer turns texts into subword units.\n",
        "* How you can use the resulting tokenizer to prepare data for training a transformer language model.\n",
        "* Which similarities an SLM captures in its token embeddings.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "* Load and inspect the **Africa Galore** dataset.\n",
        "* Train a BPE encoder on the Africa Galore dataset.\n",
        "* Encode and decode example words and sentences (including made-up words) to see how this tokenizer handles **out-of-vocabulary (OOV)** cases.\n",
        "* Convert the tokenized corpus into **padded numerical index sequences** required for model training.\n",
        "* Train the **transformer model** from the previous course on the dataset and observe how.\n",
        "* Visualize the learned embeddings of some of the tokens using the **t-SNE algorithm**.\n",
        "\n",
        "Note that you do not need to add any code to perform the activities in this notebook. All the code will run without making any modifications. For some activities, however, you may want to update parameters to see how they affect the behavior of your tokenizer and model.\n",
        "\n",
        "If you are able to, we highly recommend running the code in this lab on a **Colab instance with a GPU**. See the section \"How to use Google Colaboratory (Colab)\" below for instructions on how to do this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDWsJUGcf4Ru"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlNG_jg-39Zj"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over a cell, and click on the `run` button to its left. The run button is the circle with the triangle (â–¶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or âŒ˜+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyTT6C0JhGBs"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbtgZxrpjm6j"
      },
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order. Otherwise, the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime â†’ Run before__  from the menu above (or use the keyboard combination Ctrl/âŒ˜ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLvkcx5pUItk"
      },
      "source": [
        "### Using Colab with a GPU\n",
        "\n",
        "A **GPU** is a special type of hardware that can significantly speed up some types of computations of machine learning models. Several of the activities in this lab will also run a lot faster if you run them on a GPU.\n",
        "\n",
        "Follow these steps to run the activities in this lab on a GPU:\n",
        "\n",
        "1.  In the top menu bar, click on **Runtime**.\n",
        "2.  Select **Change runtime type** from the dropdown menu.\n",
        "3.  In the pop-up window under **Hardware Accelerator**, select **GPU** (usually listed as `T4 GPU`).\n",
        "5.  Click **Save**.\n",
        "\n",
        "Your Colab session will now restart with GPU access.\n",
        "\n",
        "Note that access to GPUs is limited and at times, you may not be able to run this lab on a GPU. All activities will still work but they will run slower and you will have to wait longer for some of the cells to finish running.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3uKWtHW7g7R"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will primarily work with the `ai_foundations` package. In the background, this implements methods for defining and training the SLM in Keras and visualizing the learned embeddings.\n",
        "\n",
        "Run the following cell to import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pf2UwzgEK4Zc"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "from collections import Counter # For counting tokens in the BPE tokenizer.\n",
        "import os # Used for setting Keras configuration variables.\n",
        "import string # For accessing string constants.\n",
        "\n",
        "# The following line provides configuration for Keras.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "import keras\n",
        "import numpy as np # For working with vectors and matrices.\n",
        "import pandas as pd # For loading the Africa Galore dataset.\n",
        "import tqdm # For displaying progress bars.\n",
        "\n",
        "from ai_foundations import training # For defining and training the SLM.\n",
        "from ai_foundations import embeddings as emb # For visualizing embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LBdln2Ep-bg"
      },
      "source": [
        "## A complete implementation of the BPE tokenizer\n",
        "\n",
        "The class `BPEWordTokenizer` below defines a complete implementation of a BPE tokenizer, including methods for training the tokenizer on a text dataset (a corpus) and translating texts between token IDs and string representations.  \n",
        "\n",
        "This class primarily contains methods that you implemented yourself in the fourth lab of this course:\n",
        "\n",
        "* `__init__`: The constructor. It builds the list of merges and the vocabulary if a dataset is specified.\n",
        "- `encode`: Converts a text to a list of token IDs.\n",
        "* `_split_text`: A helper method for `encode`. Splits a string into subword tokens using the learned list of merges.\n",
        "- `decode`: Reconstructs a text from a list of token IDs.\n",
        "* `_join_text`: A helper method for `decode`. Reconstructs text from BPE tokens.\n",
        "* `_learn_bpe`: Learns merge rules from training text.\n",
        "* `_get_pair_frequencies`: A helper method for `learn_bpe`. Returns the frequencies of all adjacent token pairs.\n",
        "* `_merge_pairs_in_word`: A helper method for `_learn_bpe` and `_split_text`. Applies a merge to an individual word represented by a list of subword tokens.\n",
        "\n",
        "Go through the implementation of `BPEWordTokenizer` to understand what each of these methods does. Then run the cell to define the `BPEWordTokenizer` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MnNmrjRJ8ak"
      },
      "outputs": [],
      "source": [
        "class BPEWordTokenizer:\n",
        "    \"\"\"\n",
        "    A Byte Pair Encoding (BPE) based subword tokenizer.\n",
        "\n",
        "    This class supports encoding and decoding text to subword tokens using BPE.\n",
        "    It can learn merge rules from a corpus or be initialized with a pre-built\n",
        "    vocabulary.\n",
        "\n",
        "    Attributes:\n",
        "        vocabulary: List of subword tokens including special tokens.\n",
        "        vocabulary_size : Total number of tokens in vocabulary.\n",
        "        token_to_index: Mapping from tokens to indices.\n",
        "        index_to_token: Mapping from indices to tokens.\n",
        "        pad_token_id: Index of the padding token.\n",
        "        unknown_token_id: Index of the unknown token.\n",
        "        tokenized_corpus: Cached tokenized corpus after BPE training.\n",
        "    \"\"\"\n",
        "\n",
        "    UNKNOWN_TOKEN = \"<UNK>\"\n",
        "    PAD_TOKEN = \"<PAD>\"\n",
        "    END_WORD = \"</w>\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: list[str] | str,\n",
        "        vocabulary: list[str] | None = None,\n",
        "        num_merges: int = 100,\n",
        "    ):\n",
        "        \"\"\"Initializes the BPEWordTokenizer.\n",
        "\n",
        "        If no vocabulary is specified, it extracts the unique tokens from the\n",
        "        text corpus and learns the BPE merges.\n",
        "\n",
        "        Args:\n",
        "          texts: A list of strings or a string representing the text corpus.\n",
        "          vocabulary: Optional list of strings with unique tokens.\n",
        "          num_merges: Defines how many rounds of merges should be performed\n",
        "            when learning the BPE merges.\n",
        "        \"\"\"\n",
        "\n",
        "        # Normalize to list of strings.\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "\n",
        "        if vocabulary is None:\n",
        "            # Learn BPE merges and derive vocabulary from tokenized corpus.\n",
        "            self.merges, tokenized, vocabulary_set = self._learn_bpe(\n",
        "                texts, num_merges\n",
        "            )\n",
        "            self.tokenized_corpus = tokenized\n",
        "\n",
        "            # Ensure that basic alphanumeric characters are always included in\n",
        "            # the vocabulary.\n",
        "            required_chars = set(\n",
        "                string.ascii_lowercase + string.ascii_uppercase + string.digits\n",
        "            )\n",
        "\n",
        "            vocabulary_set.update(required_chars)\n",
        "\n",
        "            # Add special tokens to the vocabulary.\n",
        "            self.vocabulary = (\n",
        "                [self.PAD_TOKEN] + sorted(vocabulary_set) + [self.UNKNOWN_TOKEN]\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.vocabulary = vocabulary\n",
        "            self.merges = []  # Skip merge logic when a vocabulary is provided.\n",
        "\n",
        "        # Build mappings and set IDs of special tokens.\n",
        "        self.vocabulary_size = len(self.vocabulary)\n",
        "        self.token_to_index = {tok: i for i, tok in enumerate(self.vocabulary)}\n",
        "        self.index_to_token = {i: tok for i, tok in enumerate(self.vocabulary)}\n",
        "        self.pad_token_id = self.token_to_index[self.PAD_TOKEN]\n",
        "        self.unknown_token_id = self.token_to_index[self.UNKNOWN_TOKEN]\n",
        "\n",
        "    def _split_text(self, text: str) -> list[str]:\n",
        "        \"\"\"Split a string into subword tokens using learned BPE merges.\n",
        "\n",
        "        Args:\n",
        "          text: String to split into subword tokens.\n",
        "\n",
        "        Returns:\n",
        "          List of subword tokens that together form the original text.\n",
        "        \"\"\"\n",
        "        tokens = []\n",
        "        for word in text.strip().split():\n",
        "            # Split the string into characters and add special END_WORD token.\n",
        "            chars = list(word) + [self.END_WORD]\n",
        "\n",
        "            # Merge individual characters according to learned BPE merges.\n",
        "            for pair in self.merges:\n",
        "                chars = self._merge_pairs_in_word(chars, pair)\n",
        "            tokens.extend(chars)\n",
        "        return tokens\n",
        "\n",
        "    def join_text(self, tokens: list[str]) -> str:\n",
        "        \"\"\"Join subword tokens into full string, preserving word boundaries.\n",
        "\n",
        "        Args:\n",
        "          tokens: List of subword tokens to be joined.\n",
        "\n",
        "        Returns:\n",
        "          String obtained from joining the subword tokens.\n",
        "        \"\"\"\n",
        "        words = []\n",
        "        current_word = []\n",
        "        for token in tokens:\n",
        "            # Check whether token ends with a word boundary marker.\n",
        "            if token.endswith(self.END_WORD):\n",
        "                current_word.append(token.replace(self.END_WORD, \"\"))\n",
        "                words.append(\"\".join(current_word))\n",
        "                current_word = []\n",
        "            else:\n",
        "                current_word.append(token)\n",
        "        if current_word:\n",
        "            words.append(\"\".join(current_word))\n",
        "        return \" \".join(words).strip()\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"\n",
        "        Encode a string into list of token indices.\n",
        "\n",
        "        Args:\n",
        "            text: Input text.\n",
        "\n",
        "        Returns:\n",
        "            List of integers corresponding to tokens.\n",
        "        \"\"\"\n",
        "        token_ids = []\n",
        "        for token in self._split_text(text):\n",
        "            token_id = self.token_to_index.get(token, self.unknown_token_id)\n",
        "            token_ids.append(token_id)\n",
        "        return token_ids\n",
        "\n",
        "    def decode(self, token_ids: int | list[int]) -> str:\n",
        "        \"\"\"\n",
        "        Decode list of token IDs back to original text.\n",
        "\n",
        "        Args:\n",
        "          token_ids: Single index or list of token IDs.\n",
        "\n",
        "        Returns:\n",
        "          Decoded text string.\n",
        "        \"\"\"\n",
        "        # Covert to list if a single token index is specified.\n",
        "        if isinstance(token_ids, int):\n",
        "            token_ids = [token_ids]\n",
        "\n",
        "        tokens = []\n",
        "        for token_id in token_ids:\n",
        "            tokens.append(\n",
        "                self.index_to_token.get(\n",
        "                    token_id,\n",
        "                    self.UNKNOWN_TOKEN + self.END_WORD,\n",
        "                )\n",
        "            )\n",
        "        return self.join_text(tokens)\n",
        "\n",
        "    def _get_pair_frequencies(self, corpus: list[list[str]]) -> Counter[str]:\n",
        "        \"\"\"Count all adjacent token pairs in corpus.\n",
        "\n",
        "        Args:\n",
        "          corpus: A list of lists of strings representing subword tokens.\n",
        "\n",
        "        Returns:\n",
        "          Counter mapping adjacent pairs of subword tokens to their frequencies.\n",
        "        \"\"\"\n",
        "        pairs = Counter()\n",
        "        for word in corpus:\n",
        "            for i in range(len(word) - 1):\n",
        "                pair = (word[i], word[i + 1])\n",
        "                # Increase the count by 1.\n",
        "                pairs[pair] += 1\n",
        "        return pairs\n",
        "\n",
        "    def _merge_pairs_in_word(\n",
        "        self, word: list[str], pair_to_merge: tuple[str, str]\n",
        "    ) -> list[str]:\n",
        "        \"\"\"Merge all occurrences of a token pair inside a word.\n",
        "\n",
        "        Args:\n",
        "          tokens: A list of subword tokens representing one space separated\n",
        "            word.\n",
        "          pair_to_merge: A pair of two subword tokens that should be merged into\n",
        "            one subword token.\n",
        "\n",
        "        Returns:\n",
        "          New list of subword tokens representing the word after applying the\n",
        "            merge.\n",
        "        \"\"\"\n",
        "\n",
        "        merged_symbol = pair_to_merge[0] + pair_to_merge[1]\n",
        "        if pair_to_merge[0] not in word or pair_to_merge[1] not in word:\n",
        "            return word\n",
        "        i = 0\n",
        "        new_word = []\n",
        "        while i < len(word):\n",
        "            if i < len(word) - 1 and (word[i], word[i + 1]) == pair_to_merge:\n",
        "                new_word.append(merged_symbol)\n",
        "                i += 2\n",
        "            else:\n",
        "                new_word.append(word[i])\n",
        "                i += 1\n",
        "        return new_word\n",
        "\n",
        "    def _learn_bpe(\n",
        "        self, corpus: list[str], num_merges: int\n",
        "    ) -> tuple[list[tuple[str, str]], list[list[list[str]]], set[str]]:\n",
        "        \"\"\"\n",
        "        Learn BPE merges from a corpus of texts.\n",
        "\n",
        "        Args:\n",
        "          corpus: List of input texts.\n",
        "          num_merges: Number of merge operations to perform.\n",
        "\n",
        "        Returns:\n",
        "            merges: List of merges in order they are learned to be performed.\n",
        "            tokenized_corpus: List of list of list of subword tokens where each\n",
        "              paragraph in the corpus is tokenized as a list of list of\n",
        "              subword tokens.\n",
        "            vocabulary_set: Set of subword tokens after performing all merges.\n",
        "        \"\"\"\n",
        "        # List of lists of lists to store tokenized text corpus.\n",
        "        tokenized_corpus = []\n",
        "        vocabulary = set([self.END_WORD])\n",
        "        for paragraph in corpus:\n",
        "            sentence_raw_tokens = []\n",
        "            for word in paragraph.strip().split():\n",
        "                # Split the word into characters and add word boundary marker.\n",
        "                sentence_raw_tokens.append(list(word) + [self.END_WORD])\n",
        "                vocabulary.update(list(word))\n",
        "            tokenized_corpus.append(sentence_raw_tokens)\n",
        "\n",
        "        merges = []\n",
        "        for _ in (pbar := tqdm.tqdm(range(num_merges), unit=\"merges\")):\n",
        "            # Build a one-dimensional list of all tokens in the corpus.\n",
        "            flat_corpus = []\n",
        "            for tokenized_paragraph in tokenized_corpus:\n",
        "                flat_corpus.extend(tokenized_paragraph)\n",
        "\n",
        "            # Find the most frequent pair of adjacent tokens.\n",
        "            pair_freqs = self._get_pair_frequencies(flat_corpus)\n",
        "            if not pair_freqs:\n",
        "                break\n",
        "            most_freq_pair, freq = pair_freqs.most_common(1)[0]\n",
        "            if freq < 1:\n",
        "                break\n",
        "            merges.append(most_freq_pair)\n",
        "\n",
        "            # Apply merge to each token in each paragraph.\n",
        "            new_tokenized_corpus = []\n",
        "            for para_tokens in tokenized_corpus:\n",
        "                new_para_tokens = []\n",
        "                for word_tokens in para_tokens:\n",
        "                    new_para_tokens.append(\n",
        "                        self._merge_pairs_in_word(word_tokens, most_freq_pair)\n",
        "                    )\n",
        "                new_tokenized_corpus.append(new_para_tokens)\n",
        "            tokenized_corpus = new_tokenized_corpus\n",
        "            vocabulary.add(most_freq_pair[0] + most_freq_pair[1])\n",
        "            pbar.set_postfix(vocabulary_size=f\"{len(vocabulary):,}\")\n",
        "\n",
        "        return merges, tokenized_corpus, vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2Xu099HcVl0"
      },
      "source": [
        "## Train the BPE tokenizer\n",
        "\n",
        "You will train the BPE tokenizer on the [Africa Galore](https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json) dataset.\n",
        "\n",
        "As you have learned in this course, the first step would usually be to preprocess the dataset. However, as the Africa Galore dataset was synthetically generated for this course, it does not require any additional preprocessing and you can skip this step.\n",
        "\n",
        "Run the following cell to load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUZD-iTXcURo"
      },
      "outputs": [],
      "source": [
        "africa_galore = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
        ")\n",
        "dataset = africa_galore[\"description\"].values\n",
        "print(\"Loaded dataset with\", dataset.shape[0], \"paragraphs.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YmxBl3Ddy7R"
      },
      "source": [
        "You will now use the `BPEWordTokenizer` class to learn merges from your dataset.\n",
        "\n",
        "For this dataset, performing around 3,000 merges results in tokens for many common words and subwords for common prefixes and suffixes. However, this is just a recommendation and you can experiment with increasing or decreasing the number of merges. Before you move on to the next activities, however, re-run the cell with 3,000 merges. Otherwise, some of the activities in later parts of this lab may not work.\n",
        "\n",
        "Run the following cell to train your tokenizer. As it learns to perform merges, observe how the size of the vocabulary increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEYWV2_-NC_t"
      },
      "outputs": [],
      "source": [
        "num_merges = 3000\n",
        "\n",
        "tokenizer = BPEWordTokenizer(dataset, num_merges=num_merges)\n",
        "print(f\"\\n\\nFinal tokenizer vocabulary size: {tokenizer.vocabulary_size:,}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVczlKwKuh3t"
      },
      "source": [
        "Note that the final vocabulary size is slightly higher than the vocabulary size after the last merge. This is because  the constructor (the `__init__` method) also adds special tokens for padding and the unknown token to vocabulary and makes sure that all letters, numbers and common symbols are included in the vocabulary. This means that several tokens that do not appear in the corpus are also added to the vocabulary after the merges have been learned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bgj-mhzuAdq"
      },
      "source": [
        "### Observe the behavior of your tokenizer\n",
        "\n",
        "To get a sense of what kind of tokens the vocabulary of your tokenizer includes and what merges it has learned, examine the tokenization of the first 20 words in the Africa Galore dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuiPHynwVstq"
      },
      "outputs": [],
      "source": [
        "africa_galore_tokenized = tokenizer.tokenized_corpus\n",
        "for tokens in africa_galore_tokenized[0][:20]:\n",
        "    print(\" \".join(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrDK-UfZr9cz"
      },
      "source": [
        "Each line in the output above shows a word from the original Africa Galore text, which has been broken down into its subword tokens by the BPEWordTokenizer. Notice how a word like \"launched\" is broken down into the smaller, more common subword tokens it learned during training, such as \"la\", \"un\", and \"ched`</w>`\", where the `</w>` marks the end of the word. This is the strength of the BPE algorithm. Instead of preserving whole words, it represents them as sequences of frequently occurring pieces. This approach allows the model to understand new or rare words by breaking them down into familiar parts, effectively solving the out-of-vocabulary problem while keeping the vocabulary size manageable.\n",
        "\n",
        "A key design choice when training a BPE tokenizer is to decide how many merges to perform. The right number depends on your dataset, your language, and your final goal. Consequently, finding the optimal vocabulary size is often a process of experimenting to see what gives your model the best performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc4Y6cEuOP6a"
      },
      "source": [
        "### Tokenization of unknown words\n",
        "\n",
        "To illustrate how the BPE tokenizer handles unknown words, tokenize the phrase \"A Zimbabwian dish ðŸ˜‹.\" which contains \"Zimbabwian\", a word that does not appear in the Africa Galore corpus, and the emoji \"ðŸ˜‹\".\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnMJzC6MOfwT"
      },
      "outputs": [],
      "source": [
        "madeup_tokens = tokenizer.encode(\"A Zimbabwian dish ðŸ˜‹.\")\n",
        "\n",
        "print(f\"Decoded sentence from tokens: {tokenizer.decode(madeup_tokens)}\")\n",
        "for token in madeup_tokens:\n",
        "    decoded_token = tokenizer.decode(token)\n",
        "    print(f\"Token {token}:\\t{decoded_token}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgW9jEg6RCvu"
      },
      "source": [
        "As you can see, instead of mapping \"Zimbabwian\" to the uninformative `<UNK>` token, the tokenizer splits this into the tokens \"Zimbab\", \"wi\", \"an\". If the model has been trained on the word \"Zimbabwe\" tokenized into \"Zimbab\" and \"we\", it will likely be able to relate \"Zimbabwian\" to \"Zimbabwe\".\n",
        "\n",
        "As the Africa Galore dataset does not contain any emojis and you did not manually add any Unicode characters to the vocabulary of the tokenizer, the emoji \"ðŸ˜‹\" is mapped to the `<UNK>` token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vUg54_8w71S"
      },
      "source": [
        "## Prepare the dataset for training the model\n",
        "\n",
        "With your tokenizer in place, you can now prepare the dataset for training your SLM. Recall that the data needs to be converted to token IDs so that it can serve as input for a transformer language model.\n",
        "\n",
        "As a first step, run the following cell to use your BPE tokenizer to convert the training dataset into sequences of token IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HbugFA0WweK"
      },
      "outputs": [],
      "source": [
        "encoded_tokens = []\n",
        "for paragraph in tqdm.tqdm(dataset, unit=\"paragraphs\"):\n",
        "    encoded_tokens.append(tokenizer.encode(paragraph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANMvuvzmGY1g"
      },
      "source": [
        "Next, pad all sequences. Recall that the input to a transformer model has to be a sequence of a specific length. By padding shorter sequences and truncating longer ones, you can make sure that all sequences have the length `max_length`.\n",
        "\n",
        "Run the following cell to truncate and pad sequences using the Keras function `keras.preprocessing.sequence.pad_sequences`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU7R04x9Gqqg"
      },
      "outputs": [],
      "source": [
        "max_length = 300\n",
        "padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "        encoded_tokens,\n",
        "        maxlen=max_length,\n",
        "        padding=\"post\",\n",
        "        truncating=\"post\",\n",
        "        value=tokenizer.pad_token_id,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOq1jhlAIJAn"
      },
      "source": [
        "Finally, prepare the input and the targets for the transformer model. As you are training the model on the next-word prediction task, the target for each token is the next token in the sequence.\n",
        "\n",
        "Run the following cell to create the list of input sequences and the list of target sequences by shifting the input sequence by one token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxOjSi5DJbAd"
      },
      "outputs": [],
      "source": [
        "# Prepare input and target for the transformer model.\n",
        "# For each example, extract all tokens except the last one.\n",
        "input_sequences = padded_sequences[:, :-1]\n",
        "# For each example, extract all tokens except the first one.\n",
        "target_sequences = padded_sequences[:, 1:]\n",
        "\n",
        "max_length = input_sequences.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pZxJJm1xEdk"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "You can now train the model. The following cell initializes a model and then trains it for 100 epochs following the same process you followed in 01 Build Your Own Small Language Model. Every 10 epochs, it prints a generation starting with \"Jide\" that lets you monitor how the generations change over time.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfVJESHwJqGl"
      },
      "outputs": [],
      "source": [
        "# Set a seed for reproducability.\n",
        "keras.utils.set_random_seed(3112)\n",
        "\n",
        "model = training.create_model(\n",
        "    max_length=max_length,\n",
        "    vocabulary_size=tokenizer.vocabulary_size,\n",
        "    learning_rate=8e-5\n",
        ")\n",
        "\n",
        "prompt = \"Jide\"\n",
        "prompt_ids = tokenizer.encode(prompt)\n",
        "text_gen_callback = training.TextGenerator(\n",
        "    max_tokens=11, start_tokens=prompt_ids, tokenizer=tokenizer, print_every=10\n",
        ")\n",
        "\n",
        "num_epochs = 100\n",
        "# verbose=2: Instructs the model.fit method to print one line per\n",
        "# epoch so you can observe loss decreasing and the generated texts improving.\n",
        "history = model.fit(\n",
        "    x=input_sequences,\n",
        "    y=target_sequences,\n",
        "    verbose=2,\n",
        "    epochs=num_epochs,\n",
        "    batch_size=2,\n",
        "    callbacks=[text_gen_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuPwQ7G4Kd1I"
      },
      "source": [
        "#### What did you observe?\n",
        "\n",
        "After earlier epochs, the model likely generated sequences that include words that are just random strings of characters. This is because the model now samples subword tokens instead of entire words. Early in the training process, the model has not yet learned the correct patterns for combining subword tokens. As a result, it combines them randomly, often forming strings that do not correspond to actual words.\n",
        "\n",
        "As the model learns better parameters during training, it is able to successfully combine subword tokens to form actual, recognizable words. This is a necessary step for the eventual goal of generating fluent sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC4eMGoKxG9j"
      },
      "source": [
        "## Visualize the embeddings\n",
        "\n",
        "Your small language model learns token embeddings just like the Gemma model. After you have trained your model, you are similarly able to visualize them.\n",
        "\n",
        "The following cell extracts the embeddings from your model and then selects several tokens corresponding to different word categories (food and drink related words, prepositions, adjectives, and country names). It then uses t-SNE to project the embeddings to a 2-dimensional space, so that they can be plotted.\n",
        "\n",
        "Run the following cell to generate the t-SNE plot and investigate which clusters emerge in this plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3siUnnUKUA2-"
      },
      "outputs": [],
      "source": [
        "# Extract all embeddings from your model as a matrix.\n",
        "embeddings = model.trainable_weights[0].value\n",
        "\n",
        "# Define token categories.\n",
        "food_and_drink = [\n",
        "    \"water\",\n",
        "    \"coffee\",\n",
        "    \"onions\",\n",
        "    \"peanut\",\n",
        "    \"pepper\",\n",
        "    \"pudding\",\n",
        "    \"sauce\",\n",
        "    \"stew\",\n",
        "    \"carrots,\"\n",
        "]\n",
        "prepositions = [\"on\", \"in\", \"with\", \"for\", \"of\"]\n",
        "adjectives = [\"aromatic\", \"hot\"]\n",
        "countries = [\"Egypt\", \"Congo\", \"Ghana,\", \"Tanzania\"]\n",
        "\n",
        "# Define list of tokens and map them to categories for coloring them.\n",
        "tokens = food_and_drink + prepositions + adjectives + countries\n",
        "categories = (\n",
        "    [0] * len(food_and_drink)\n",
        "    + [1] * len(prepositions)\n",
        "    + [2] * len(adjectives)\n",
        "    + [3] * len(countries)\n",
        ")\n",
        "\n",
        "# Convert tokens into token IDs.\n",
        "token_ids = []\n",
        "for token in tokens:\n",
        "    token_ids.extend(tokenizer.encode(token))\n",
        "\n",
        "# Check that each token is represented as a single token in the tokenizer.\n",
        "assert len(token_ids) == len(tokens)\n",
        "\n",
        "# Extract embeddings for the set of tokens of interest.\n",
        "embeddings_subset = embeddings[token_ids, :]\n",
        "\n",
        "# Generate t-SNE plot with embeddings from your model.\n",
        "emb.plot_embeddings_tsne(embeddings_subset, tokens, categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb_ByK9hPIi_"
      },
      "source": [
        "#### What did you observe?\n",
        "\n",
        "In the plot above, you may have observed that the prepositions (in blue) form a relatively tight cluster. Most of the other terms are likely quite close in space as well but they may not form as tight clusters.\n",
        "\n",
        "This is because the model that you trained was trained on a very small dataset and some of these tokens appear in very few contexts. Therefore, the model may have not had very much information to learn the meaning of some of these words. On the other hand, for frequent function words, like prepositions, even such a small dataset provides a lot of information. This means the model learns that they have similar meanings and serve similar functions in sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b771e661"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this lab, you **trained a small language model** from scratch by first creating and training a custom **byte-pair encoding (BPE) tokenizer** on the Africa Galore dataset. You used this tokenizer to convert text into **subword tokens** and prepare the data for training. After training the transformer model, you observed how its ability to generate coherent text improved as it learned to combine these subword tokens into meaningful words and sentences. Finally, by visualizing the model's learned **embeddings** with t-SNE, you explored how it began to organize tokens into meaningful clusters. You observed how it grouped words with similar functions (e.g., prepositions) together in the embedding space."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NDWsJUGcf4Ru"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
